{"cells":[{"cell_type":"markdown","source":["### This Notebook serves 2 purposes with Apache Spark\n1. It provides an analysis of the Movie Lens Dataset specificaly the movies based on their times and Genres\n2. Recommendation engine based on Collabrative filtering\n\nSome of the Links and references I have used are -\n1. Dataset can be found [here](https://grouplens.org/datasets/movielens/20m/) or at [kaggle](https://www.kaggle.com/grouplens/movielens-20m-dataset)\n2. edX Apache Spark course cs110 Assignment 2 (Movie Recommendation) I have used my own code from that notebook (Not providing my notebook link here just in case course is offered again with same Assignments) Side note -  It is one of the best and the most intense course series I have ever done.\n3. A More Scalable Way of Making Recommendations with MLlib - Xiangrui Meng [here](https://www.youtube.com/watch?v=Q0VXllYilM0&)\n4. I have used [Databricks community edition cloud](https://community.cloud.databricks.com). This is because we already have the required dataset mounted on the Cloud. Also, Databricks community edition has tons of features (I love the display feature) and the whole system is preconfigured.One important thing here (Also, a differentiating feature) is we do not need to create spark context or sql context object which is already created for us\n\nA caution Note here- \nWe won't do collect() here as that will push all the data back to the Driver which might cause out of memory error."],"metadata":{}},{"cell_type":"code","source":["#We already have sc and sqlContext for us here\nprint sc\nprint sqlContext"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Getting the data\nIt is already mounted for us"],"metadata":{}},{"cell_type":"code","source":["import os\nfrom databricks_test_helper import Test\n\ndbfs_dir = '/databricks-datasets/cs110x/ml-20m/data-001'\n\n#We will use these 2 files for our analysis and collabrative filtering\nratings_filename = dbfs_dir + '/ratings.csv' \nmovies_filename = dbfs_dir + '/movies.csv'"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#This is a databricks feature\ndisplay(dbutils.fs.ls(dbfs_dir))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["dbutils.fs.head(movies_filename)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### A Little analysis on the movies.csv\nWe will create 2 dataframes for our analysis which will make the visualization with Databricks display function pretty straightforward- \n1. movies_based_on_time - We will drop the genres here final schema will be (movie_id,name, Year)\n2. movies_based_on_genres - Final schema would look like (movie_id,name_with_year,one_genre)\n\nFrom the description at [kaggle](https://www.kaggle.com/grouplens/movielens-20m-dataset) we can see the schema of both the files. for the sake of computation we would explicitly mention the schema(Spark can infer it itself but that involves an action which at most cases we want to minimize)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n#working only on movies.csv right now\nmovies_with_genres_df_schema = StructType(\n  [StructField('ID', IntegerType()),\n   StructField('title', StringType()),\n   StructField('genres',StringType())]\n  )\n\nmovies_df_schema = StructType(\n  [StructField('ID', IntegerType()),\n   StructField('title', StringType())]\n  ) #dropping the genres.Also, we will tranform the df to include the Year later"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#Creating the dataframes \nraw_movies_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(movies_df_schema).load(movies_filename)\nmovies_with_genres_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(movies_with_genres_df_schema).load(movies_filename)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Inspecting the DataFrames before the transformations"],"metadata":{}},{"cell_type":"code","source":["raw_movies_df.show(4,truncate = False) #we will also use this for Collabrative filtering\nmovies_with_genres_df.show(4,truncate = False)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#transforming the Dataframes\nfrom pyspark.sql.functions import split, regexp_extract\n\n# Side note a very nice quote -- Some people, when confronted with a problem, think \"I know, I'll use regular expressions.\" Now they have two problems.(attributed to Jamie #Zawinski)\nmovies_with_year_df = raw_movies_df.select('ID','title',regexp_extract('title',r'\\((\\d+)\\)',1).alias('year'))\n\n#one genre per row\nmovies_with_one_genre_df = sqlContext.createDataFrame(movies_with_genres_df.rdd.map(lambda (a,b,c): [(a,b,i) for i in c.split('|')])\\\n                                                      .flatMap(lambda x:x)).toDF('Id','title','one_genre') "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### DataFrames after Transformation"],"metadata":{}},{"cell_type":"code","source":["movies_with_one_genre_df.show(10,truncate = False)\nmovies_with_year_df.show(4,truncate = False)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Now we will use the inbuilt functionality of Databricks for some insights"],"metadata":{}},{"cell_type":"code","source":["display(movies_with_one_genre_df.groupBy('one_genre').count()) #people love drama\n\n#Below we have a bar chart here we can choose from a lot of other options"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#from here we can look at the count and find that the maximum number of movies are produced in 2009\ndisplay(movies_with_year_df.groupBy('year').count().orderBy('count',ascending = False))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Creating Data Frames for the rating and movies\nFrom the description at [kaggle](https://www.kaggle.com/grouplens/movielens-20m-dataset) we can see the schema of both the files. for the sake of computation we would explicitly mention the schema(Spark can infer it itself but that involves an action which at most cases we want to minimize)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nratings_df_schema = StructType(\n  [StructField('userId', IntegerType()),\n   StructField('movieId', IntegerType()),\n   StructField('rating', DoubleType())]\n)              #we are dropping the Time Stamp column\nmovies_df_schema = StructType(\n  [StructField('ID', IntegerType()),\n   StructField('title', StringType())]\n) #dropping genres here\n"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"Movie_Lens_20M","notebookId":3123150065672593},"nbformat":4,"nbformat_minor":0}
